import igraph as ig
import leidenalg as la
from numpy import argsort, array
from sklearn.feature_extraction.text import TfidfVectorizer

from raw_db_access import get_citations_pairs, get_papers


def create_paper_partitions(papers, citations, resolution_parameter=1,
                            n_iterations=2, max_comm_size=0):
    """Creates partitions of papers using the Leiden algorithm
    papers - papers used to create vertices of a graph
    citations - citations used to create edges of a graph
    resolution_parameter - resolution_parameter for Leiden alg
    n_iterations - number of iterations of Leiden alg
    max_comm_size - maximum size of created communities
    Returns a list containing lists of lists representing papers
    belonging to specific clusters"""
    graph = ig.Graph.DictList(papers, citations, directed=False)
    partition = la.find_partition(graph, la.RBConfigurationVertexPartition,
                                  resolution_parameter=resolution_parameter,
                                  n_iterations=n_iterations,
                                  max_comm_size=max_comm_size)
    partition_list = list(partition)
    return partition_list


def extract_ids_from_clusters(clusters, papers):
    """Converts clusters formed of papers array indices to clusters
    consisting of paper ids
    clusters- lists of lists with indices of papers array signifying clusters
    papers - paper data in a form list of dictionaries containing names, titles
    and abstracts of papers
    Returns list of lists filled with papers ids. Each list withing a list
    symbolizes a single cluster."""
    id_list = []
    for p in clusters:
        id_list.append(list(map(lambda i: papers[i]['name'], p)))
    return id_list


def aggregate_abstracts_titles(paper_data, paper_id_clusters):
    """Aggregates abstracts and titles of publications comprising
    specific clusters
    paper_data - paper data in a form of a list of
    dictionaries containing ids, titles and abstracts
    paper_id_clusters - lists containing lists filled with paper_ids
    Returns lists of strings combining titles and abstracts of publications
    of each clusters
    """
    clustered_documents = []
    for cluster in paper_id_clusters:
        words = ''
        for paper_id in cluster:
            paper = paper_data[paper_id]
            words = words + ' ' + paper.get('title') + ' ' + paper.get(
                'abstract')
        clustered_documents.append(words)
    return clustered_documents


def get_tfidf_matrix(documents,
                     stop_words=None, min_df=10,
                     max_df=1.0, max_features=None):
    """Calculates weights of keywords in documents suing tf-idf method
    documents - list containing strings representing documents
    for tf-idf verctorizer
    stop_words - stop_words for the tf-idf algorithm
    min_df - min_df parameter for tf-idf vectorizer
    max_df - max_df parameter for tf-idf vectorizer
    max_features - maximum number of features
    Returns keyword matrix generated by the vecotrizer and names of the
    columns (features)"""
    if len(documents) < min_df:
        # If number of documents lower than min_df
        min_df = 1
    vectorizer = TfidfVectorizer(lowercase=False, stop_words=stop_words,
                                 min_df=min_df, max_df=max_df,
                                 max_features=max_features)
    keyword_matrix = vectorizer.fit_transform(documents)
    feature_names = vectorizer.get_feature_names()
    # vocabulary = vectorizer.vocabulary
    return keyword_matrix, feature_names


def get_top_tf_idf_words(row, feature_names, top_n=25):
    """Selects top_n keywords with the highest weight in a row
    row - sparse matrix with one row and columns
    representing weights of keywords
    feature_names - names of the features
    top_n - number of keywords to choose
    Returns an array with the selected keywords"""

    # Code comes from https://stackoverflow.com/questions/34232190/
    # scikit-learn-tfidfvectorizer-
    # how-to-get-top-n-terms-with-highest-tf-idf-score
    feature_names_arr = array(feature_names)
    sorted_nzs = argsort(row.data)[:-(top_n + 1):-1]
    return feature_names_arr[row.indices[sorted_nzs]]


def assign_publications_to_keywords(cluster_tuples, feature_names):
    """Assigns publications to the keywords that they are characterized with
    cluster_tuples - list of tuples with list of paper_ids in the first
    position and list of feature on the second
    feature_names - list of feature names
    Returns dictionary with words mapped to list of publications they are
    related to"""
    keyword_pubs_dict = dict()
    for cl_tuple in cluster_tuples:
        top_words = get_top_tf_idf_words(cl_tuple[1], feature_names)
        for word in top_words:
            if word not in keyword_pubs_dict.keys():
                keyword_pubs_dict[word] = []
            keyword_pubs_dict[word].extend(cl_tuple[0])
    return keyword_pubs_dict


def assign_feature_weights_to_clusters(paper_id_clusters, keyword_matrix):
    """Assigns weights of keywords to each cluster
    paper_id_clusters - list containing lists of papers and
    keyword_matrix - sparse matrix resulting from TF-IDF
    Returns list of tuples with list of paper_ids in the first position
    and list of features on the second"""
    final_cluster_tuples = []
    for count, cluster in enumerate(paper_id_clusters):
        final_cluster_tuples.append(
            (cluster, keyword_matrix.getrow(count)))
    print('assigning weights finished')
    return final_cluster_tuples


def create_clusters_assign_features(db_name, max_comm_size=0, stop_words=None):
    """Creates clusters of papers and assigns features to them
    db_name - name of the database,
    max_comm_size - maximum number of communities for clusters
    (0 means there is no limit)
    stop_words - list of stop words for the TF-IDF algorithm
    Returns list of tuples containing publications
    inside of a clusters and the words in them
    Also returns list naming with names of the columns in the keyword matrix"""

    papers = get_papers(db_name)
    clusters = create_paper_partitions(papers,
                                       get_citations_pairs(db_name),
                                       max_comm_size=max_comm_size)
    paper_id_clusters = extract_ids_from_clusters(clusters, papers)
    clustered_documents = aggregate_abstracts_titles(papers,
                                                     clusters)
    keyword_matrix, feature_names = get_tfidf_matrix(clustered_documents,
                                                     stop_words=stop_words)
    final_cluster_tuples = assign_feature_weights_to_clusters(
        paper_id_clusters,
        keyword_matrix)
    return final_cluster_tuples, feature_names


def get_keywords_with_publications(db_name):
    """Runs the complete hot keywords algorithm.
    db_name - name of the database,
    Returns dictionary mapping keywords to the list of
    publications they represent"""
    cluster_tuples, feature_names = create_clusters_assign_features(db_name,
                                                                    100,
                                                                    'english')
    print(cluster_tuples)
    keyword_pubs_dict = assign_publications_to_keywords(cluster_tuples,
                                                        feature_names)

    return keyword_pubs_dict


if __name__ == '__main__':
    # For testing purposes
    print(get_keywords_with_publications('test_aip'))
